## 基本无害的（大量）数据食用技巧指北
- 作者：小林同学（21级大数据）

### 目录
- [基本无害的（大量）数据食用技巧指北](#基本无害的大量数据食用技巧指北)
  - [目录](#目录)
  - [说明](#说明)
  - [小林的碎碎念](#小林的碎碎念)
  - [炫酷的进度条](#炫酷的进度条)
  - [养成持久化的好习惯](#养成持久化的好习惯)
  - [我的文件很大，一口气存不下](#我的文件很大一口气存不下)
  - [我的代码很慢](#我的代码很慢)
    - [缓存修饰器](#缓存修饰器)
    - [Numba提供的修饰器](#numba提供的修饰器)
    - [一行搞定并行化](#一行搞定并行化)
  - [这些技巧懒得分类](#这些技巧懒得分类)
    - [稀疏矩阵](#稀疏矩阵)
    - [巧算N-gram平滑操作](#巧算n-gram平滑操作)


### 说明
- 基于第一次NLP作业而展开。会介绍小林同学在第一次NLP作业中使用的一些技巧。这些技巧帮助小林同学在一台古董轻薄本上也能丝滑完成第一次NLP作业。
- 本篇指北介绍的是“技巧”而非技术。主要包括一些处理经验、处理思路，还会介绍几个亲测有效开箱即用的Python库。
- 不会介绍分布式数据库（但会简单提一下）、压缩传输等硬核知识点。我也不太懂嘛。
- 本篇指北希望能从一个NLP作业出发，反映并解决一些共性问题，希望读者未来遇到真正海量数据的时候，能够做到武器在手，从容不迫。
- 这也是小林同学在大数据学院的最后一个学期，愿尽自己的微薄之力，祝大数据学院越办越好。
- 也欢迎读者批评指正。可以通过邮箱联系我：[linzk21@m.fudan.edu.cn](linzk21@m.fudan.edu.cn )


### 小林的碎碎念
- 第一次NLP作业要求同学们对10000个带有分类标签的维基百科网页文本进行预处理、清洗，并基于清洗好的文本，手搓N-gram模型，并建立朴素贝叶斯和逻辑回归判别模型进行分类任务。
- 与部分朋友交流的时候，我发现很多同学在遇到了类似的问题，常见的问题包括：
  - 文件太大，内存不够，文件读到一半/存到一半，电脑先崩溃了
  - 终于把文件读进来了，结果算法运行很慢，也不知道处理到什么时候
  - 只好等算法跑一晚上，早上醒来才发现结果不对
  - 气到骂人，气到退课
  - 恼羞成怒，大喊财大牛逼
- 以上都是很常见的问题。我们虽贵为（存疑）大数据学院数据科学与大数据技术专业的优秀学生（存疑），然而我们的课程并没有教给我们应付真正大量数据的技巧。目前本人能够想到的涉及存储或处理大量数据的知识点，散见于邵老师数值一的Krylov子空间方法，江老师最优化的BFGS拟牛顿法（通过SMW公式近似矩阵逆），以及郑老师图算法的稀疏矩阵存储技术——可以说不太成体系。我们大部分的大数据处理技巧，其实一般都是在PJ，或者科研，或者实习中，误打误撞学到。
- 这样的处理水平是与真实世界脱钩的。以本人目前在某量化公司实习的经验为例：深交所和沪交所的逐笔盘口数据，每10毫秒对每只股票更新一次，每次更新的数据大概有几十笔委托单与交易数据。请注意，中国有大约五千支股票，这些数据从2020年左右开始进行流式存储。也就是说，中国A股**仅仅一天**的逐笔盘口数据便可能高达**10的11次方**，而一年的数据则会达到**10的13次方**！！面对这种级别的数据，如果没有丝滑优雅的处理技巧，甚至在文件读取阶段，就可能把公司服务器干趴。事实上，我的同事就曾不小心把公司服务器干趴。
- 与真实世界相比，我们遇到的NLP作业并不算真正意义上的大数据。毕竟我用了一台古董联想轻薄本，也能丝滑且高效地完成这次NLP作业。但这次作业中，仍然反映出大数据学院在大数据存储管理技巧的教学上些许问题。
- **以下是正文，让我们说中文。**


### 炫酷的进度条
- 当你处理的数据在短时间内无法处理完毕，你就应该考虑使用进度条，帮助你估计完成时间，**同时排除程序因为bug而卡死的可能性**。
- 进度条看起来长成这样：
```
Pre-calculating prefix count: 100%|██████████| 5931025/5931025 [00:03<00:00, 1716150.42it/s]
Calculating perplexity: 100%|██████████| 205637/205637 [00:08<00:00, 25298.18it/s]
Bigram perplexity: ****
```
- 进度条的使用方式很简单：
```python
from tqdm import tqdm

task_list = ...

for task in tqdm(task_list, total=len(task_list), desc="Processing:"):
    ...
```
- 如果你训练的是神经网络，你还可以通过`set_postfix()`方法传入每个批次的损失函数情况。
- 补充：`tqdm`也是小林同学第一个教会同事的python技巧（嘿嘿）

### 养成持久化的好习惯
- 当你处理的数据量非常大，而且中间结果具有一定的意义，你就应该考虑对部分中间结果进行持久化。
- 当你无法保证你的程序能够平安顺利运行到最后，你也应该保存一些中间结果，从而在程序崩溃的时候，从中间结果起手。
- 如果你需要大量时间才能得到你的最终结果，你也应该及时保存最终结果。
- 例如，**在本次作业中，预处理完的维基百科文本，训练好的N-gram模型，通过交叉验证选择出的最优逻辑回归模型等，都是值得持久化的中间结果/最终结果**。
- 持久化的另一个典型场合是神经网络训练，一般和早停策略结合，将训练过程中在验证集上最优的模型权重保存。如果你希望了解损失函数和参数的变化情况，也可以每间隔几个批次就保存一次模型权重。
- 持久化一般通过`pickle`和`joblib`进行。他们的使用方式是：
```python
# pickle 使用方法
import pickle

## 存储
with open("your_file_path.pkl", "wb") as f:
    pickle.dump(your_file, f)
## 读取
with open("your_file_path.pkl", "rb") as f:
    your_file = pickle.load(f)

```
```python
# joblib使用方法
import joblib

## 存储
joblib.dump(your_file, "your_file_path.pkl")
## 读取
your_file = joblib.load("your_file_path.pkl")
```
- 可以看出，用`joblib`更加简单。事实上，`joblib`也更高效，效率也更高。
- 推荐优先使用`joblib`。

### 我的文件很大，一口气存不下
- `pickle`不适合存储大型文件。如果你要存大文件，可以直接使用`joblib`。
- 我的3-gram模型是一个字典，形式是`trigram[(w1,w2,w3)] = p`。尽管我做了一些压缩，对于没有出现过的三元组`(w1,w2,w3)`不进行存储，只有用到了才做计算，使得我的3-gram的参数量远小于$\mathcal{O}(|V|^3)$，然而参数量仍然很大，即便通过`joblib`也无法正常存储，会把内存干爆。
- 那这个作业就不做了吗？
- 后来与AI反复交流，受到了一些启发：应该在计算3-gram的时候，就**流式**地将每个三元组的概率一行一行地写入`txt`文件，而不要将整个字典一口气通过`joblib`写入`pkl`文件。类似的，读取3-gram模型的时候，也是一行一行地从`txt`文件中读取经过测试，这种流式处理的方式有以下两个优点：
  - 能够丝滑存下3-gram模型
  - 之后使用的时候，读取3-gram的速度非常快
- 我的3-gram代码实现如下（声明：借鉴了AI的提示）：
```python
from collections import Counter

def build_trigram_model(alpha=1):
    
    model_path = "trigram_model.txt"
    
    if os.path.exists(model_path):
        print("Find the saved trigram model!")
        return model_path
    
    vocab_size = len(vocab) + 1 # for UNK token
    trigram_count = Counter()
    bigram_count = Counter()

    for sentence in tqdm(sentences_processed, total=len(sentences_processed), desc="Counting trigrams and bigrams"):
        for i in range(len(sentence) - 2):
            trigram = (sentence[i], sentence[i + 1], sentence[i + 2])
            bigram = (sentence[i], sentence[i + 1])
            trigram_count[trigram] += 1
            bigram_count[bigram] += 1

    # NOTE: Here I use a Streaming approach. 
    # Otherwise it will cause memory error!!
    with open(model_path, "w") as f:
        for trigram, count in tqdm(trigram_count.items(), total=len(trigram_count), desc="Calculating and saving trigram probabilities"):
            prev_bigram = (trigram[0], trigram[1])
            prob = (count + alpha) / (bigram_count[prev_bigram] + alpha * vocab_size)
            trigram_str = " ".join(trigram)
            f.write(f"{trigram_str} {prob}\n")

    print("Successfully saved trigram probabilities!")
    return model_path

def load_trigram_model(file_path):
    trigram_probs = {}
    with open(file_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            trigram = tuple(parts[:3])
            prob = float(parts[3])
            trigram_probs[trigram] = prob
    return trigram_probs


trigram_path = build_trigram_model(alpha=1)
trigram_model = load_trigram_model(trigram_path)
```
- 在我的笔记本上做了如下测试：我的1-gram和2-gram使用`pickle`进行序列化存储，而3-gram使用`txt`逐行存储。读取1-gram需要0.1秒，读取2-gram需要22秒。按照参数量比例估计，如果用`pickle`存储3-gram，则3-gram读取的时间可能会高达数千秒。但是使用`txt`流式存储3-gram，**读取的时间只需要28秒**。并且存储3-gram的时候没有干爆内存，非常丝滑。

### 我的代码很慢
- 这是一个重要的问题。**这次的NLP作业如果处理不当，可能花一个小时都跑不完代码！！**
- **你应该依次考虑的技巧包括**：使用缓存修饰器，使用`@njit`装饰器，以及使用并行化处理。尽管有时候这些技巧不一定适用于你的问题。
- 此外，还有一些更深刻的原则——我个人认为最重要的是，不要重复计算，使用一些容器存储你的各种结果，最好能一次性解决问题。但这个部分不属于指北的范围。
- 让我们从易到难，依次介绍。

#### 缓存修饰器
- 反复调用，且中间结果可能可以复用的函数，一般建议加上缓存修饰器，它的用法是：
```python
from cachetools import cached, TTLCache

@cached(cache=TTLCache(maxsize=10000, ttl=3600)) 
def process_your_task(task):
    ...

for task in task_list:
    process_your_task(task)
```
- 遗憾的是，帮大家试过了，在这次NLP作业中，使用缓存修饰器不会提高运行速度。
- 缓存修饰器有很多种，有基于停留时间的（TTL）、基于最近使用时间的（LRU）等。读者可以根据任务场景选择。

#### Numba提供的修饰器
- `numba`提供的修饰器能够把你的代码加速到接近C语言的速度。原理也很简单：尽量不使用Python的解释器。
- 一种常见的使用方法是：
``` python
import numba
import numpy as np

@numba.njit
def your_func(your_array):
    ...

your_func(your_array)
```
- `@njit`可以理解为完全不使用Python解释器。此外，还有`@jit`之类的修饰器。
- **重要提醒**：`numba`库提供的修饰器并不兼容所有的Python代码。**numba一般适用于数值计算密集、频繁使用numpy、for循环非常多的场景。numba对于Pandas等库并不兼容！！**
- 同样遗憾的是，帮大家试过了，本次作业中也没有特别适合使用`numba`的场景。

#### 一行搞定并行化
- **当你的程序因为有非常多的for循环从而非常慢，并且不同的for循环之间没有关联**，你就可以考虑并行化编程。
- 例如，本次作业的第一项任务，要统计每个类别下的网页平均单词数，必然会涉及到计算每个网页的单词数。注意到每个网页的处理是可以独立进行的，处理过程中不需要产生交互，因此这就是一个把你的for循环改成并行的典型场景。
- 并行化有多线程和多进程两种。具体可以根据你的IO情况以及CPU使用密集程度决定。**但一般使用多进程都足够很好地解决你的问题**。所以接下来我们关注多进程。
- 一种比较传统的方法是使用`multiprocessing`库
```python
from multiprocessing import Pool 
from tqdm import tqdm

def your_func():
    ...

if __name__ == "__main__":
    task_list = []
    with Pool() as pool:
        results = list(tqdm(pool.imap(your_func, task_list), total=len(task_list)))
```
- 其中的`imap`方法相比`map`方法，**有助于节省内存**。此外，results中的结果的顺序和你的`task_list`的顺序是一致的。
- 请注意，使用`multiprocessing`进行并行化处理，一般要放在`if __name__ == "__main__":` 这个语句后，不然可能会出问题，这个和python底层的一些进程管理机制有关。
- 另一个我本人更喜欢的并行化写法是使用`joblib`提供的`Parallel`和`delayed`:
```python
from joblib import Parallel, delayed
results = Parallel(n_jobs=-1)(
    delayed(your_func)(task) for task in tqdm(task_list, total=len(task_list))
)
```
- 它同样可以保证返回结果的顺序与输入一致。而且它的写法更加干净简洁，一行就能搞定。
- 本次作业中还有一个可以使用并行化处理的地方，但可能会被某些读者忽略——通过交叉验证寻找逻辑回归模型最优正则化超参数，可以使用`sklearn`自带的`GridSearchCV`类，通过设置`n_jobs`，可以进行并行搜索，我的代码如下：
```python
# Logistic Regression
model = LogisticRegression()

# We use grid search to find the best hyperparameter C of regularization
print("Start grid search for hyperparameter C of regularization")
parameters = {"C": [0.1, 1, 10, 20]}  # The best is 20
grid_search = GridSearchCV(
    model,
    parameters,
    cv=5,
    scoring="f1_macro",
    verbose=3,
    n_jobs=-1,  # Parrelel processing
)
grid_search.fit(X_train, y_train)

print("Best hyperparameter C of regularization:", grid_search.best_params_)

```
- **重要提醒：** 由于并行处理的过程中涉及到内存的分配和回收，这同样会消耗时间，因此每个并行任务的大小需要适中，尤其不要太小。例如，在NLP的第一次作业中，如果你的函数的处理对象是句子，那么并行化的效率会非常低；但如果你的函数的处理对象是一个网页中的文本，那么并行化的效率就会非常高。
- 例如，在我的古董轻薄本中进行本次NLP作业的第一项任务，统计每个类别下的网页的平均句子数和单词数，我的函数以每个网页的文本作为处理对象，并行化处理与串行化处理相比，速度提升了约5倍。
- **一点点补充**：目前的并行化处理思路是分割文件，然后并行地把这些文件移动到内存中，再由算法进行处理。一个相反的想法是，能否不要让这些文件移动，而是把算法移动到文件上去执行？完全可以！“把文件移动到算法上”是大数据时代之前的单机处理思路，而“把算法移动到文件上”则是大数据时代的分布式处理思路，主要适用于分布式数据库上的数据处理，代表的项目有*Hadoop*和*Spark*，有兴趣的读者可以进一步了解。附上一份可直接食用的光速入门资料：[大数据技术入门精讲（Hadoop+Spark）](https://www.bilibili.com/video/BV1y7421o7Na/?share_source=copy_web&vd_source=1a2aa83d368e60e53978964346e3b166&t=0)
- **一点进阶内容**：仔细观察这个第一次作业，会发现有一些环节具有上下游关系，进一步又可以分为“不用等”和“要等”。比如，预处理完后的每一个文本，可以进入下游进行词频统计，从而找到高频词，词频统计不需要等到所有文本都被预处理后才开始，这就属于“不用等”。而将文本中的低频词替换为“UNK”，则需要等到词频统计结束后才能开始，因此就是“要等”的。读者如果有图算法基础，可以结合 *PERT网络分析法* 理解一下这个问题。对于“不用等”的上下游任务，可以在并行的基础上加入 **“生产者-消费者”** 模式，上游的任务作为生产者，预处理后的文本放入一个缓冲队列，然后下游的词频统计任务从这个缓冲队列里拿取文本进行统计，从而使上下游任务同时进行，进一步提升并行效率——不过这个技巧对于本次任务意义不大，词频统计非常快。（这个技巧在小林同学（好像要难产了的）毕设中用到过。）

### 这些技巧懒得分类
- 这里是一些奇奇怪怪的小技巧，但我实在不知道该怎么分类

#### 稀疏矩阵
- 对于稀疏矩阵，读者如果了解过Krylov子空间迭代类的方法，或许不陌生。当矩阵太稀疏，直接存储并不合算。
- 大型稀疏矩阵很难进行方程组求解、矩阵求逆这一类的运算，不过做矩阵-向量乘法还是可行的。
- 常见的稀疏矩阵存储方法包括`csc`和`csr`，对于稀疏矩阵的压缩效果非常好。有兴趣的同学可以修读郑卫国老师的《图数据管理与挖掘》课程，会有比较深入的案例介绍。
- 提到稀疏矩阵的原因，主要是想讨论一下周老师提供的`Exercise-2`的代码。周老师的代码，在构建词频矩阵的过程中，用的是稠密矩阵的构建方案：
```python
# NOTE: the following codes are from Professor Zhou's solution for Exercise-2

# To represent test data as word vector counts
X_test_dataset = np.zeros((len(X_test),len(features)))
# This can take some time to complete
for i in range(len(X_test)):
    # print(i) # Uncomment to see progress
    word_list = [ word.strip(string.punctuation).lower() for word in X_test[i][1].split()]
    for word in word_list:
        if word in features:
            X_test_dataset[i][features.index(word)] += 1
```
- 然而，词频矩阵往往是比较稀疏的。我相信周老师提供的代码更多是处于教学上的考虑，帮助大家理解词频矩阵的构建原理，防止大家变成调包侠。不过，从性能上来说，我个人会更加推荐直接使用`sklearn`提供的`CountVectorizer`类，因为它用的是稀疏矩阵的`csr`存储方式，可以节省内存，尤其是我们的样本矩阵$X$大约有$\mathcal{O}(d\times|V|)$的大小，其中$d=9000$，而$|V|$即便进行了频率截断，大约也在70000左右，使用稀疏矩阵是更加合算的。
- 如果熟悉最优化的读者（你大概率应该挺熟悉吧）也可以思考一下，在对逻辑回归的参数通过梯度下降进行学习的过程中，经过求导，梯度下降公式中涉及到样本矩阵$X$的操作，是不是只有矩阵-向量乘法呢？（你要不要动手推推看，说不定最优化要考呢？）
- 嘿嘿，是不是和Krylov子空间迭代的场景很类似？
  
#### 巧算N-gram平滑操作
- 我们作业中，对N-gram的平滑方法是：
    \[
        q_{Add}(x_n|x_{1:N-1}) = \frac{1+\text{Count}(x_{1:N})}{|V| + \text{Count}(x_{1:N-1})}
        \]
- 拉普拉斯平滑方法可以帮助N-gram应对那些从未见过的N元单词组合。
- 在训练拉普拉斯平滑N-gram的过程中，一个提高效率的方法是，额外存储所有见过的$x_{1:N-1}$的频数。当在验证集上测试N-gram的时候，一旦出现从未见过的N元单词组合，就直接调取对应的(N-1)元前缀$x_{1:N-1}$的出现频数，进行Laplace平滑。
- 这样可以既避免存储$\mathcal{O}(|V|^N)$级别的参数，又能避免在遇到未见过的N元单词组合时N-gram出错。
- **补充**：有贝叶斯统计统计功底的同学应该可以看出来，Laplace平滑其实类似于使用了参数为$(1,|V|+1)$Beta先验，对Bernoulli分布的参数$q$进行后验估计。
